{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labeled_set():\n",
    "    train_set = pd.DataFrame({'Files': 0, 'Labels': 0}, index=[0]).iloc[0:0]\n",
    "    categories = os.listdir('debug_train')\n",
    "    for category in categories:\n",
    "        files = os.listdir('debug_train/' + category)\n",
    "        labels = [category] * len(files)\n",
    "        idx = [i for i in range(len(files))]\n",
    "        temp_df = pd.DataFrame({'Files': files, 'Labels': labels}, index=idx)\n",
    "        train_set = train_set.append(temp_df, ignore_index=True)\n",
    "    train_set.to_csv('Train_List.csv')\n",
    "    possible_labels = train_set['Labels'].unique()\n",
    "    return possible_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_data(file_list, percent_validation):\n",
    "    train_rows = 0\n",
    "    valid_rows = 0\n",
    "    counter = 0\n",
    "    with open(file_list, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row)>0:\n",
    "                if counter==0:\n",
    "                    with open('Train_Partition.csv', 'w') as trainfile:\n",
    "                        pass\n",
    "                    with open('Valid_Partition.csv', 'w') as validfile:\n",
    "                        pass\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    group = np.random.multinomial(1,[1.-percent_validation,\n",
    "                                                     percent_validation])\n",
    "                    counter += 1\n",
    "                    \n",
    "                    if np.argmax(group)==0:\n",
    "                        with open('Train_Partition.csv', 'a',\n",
    "                                  newline = None) as trainfile:\n",
    "                            trainwriter = csv.writer(trainfile, delimiter=',')\n",
    "                            trainwriter.writerow(row)\n",
    "                        train_rows += 1\n",
    "                    \n",
    "                    if np.argmax(group)==1:\n",
    "                        with open('Valid_Partition.csv', 'a',\n",
    "                                  newline = None) as validfile:\n",
    "                            validwriter = csv.writer(validfile, delimiter=',')\n",
    "                            validwriter.writerow(row)  \n",
    "    \n",
    "    return train_rows, valid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labeled_image_list(image_list_file):\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    counter = 0\n",
    "    with open(image_list_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row)>0:\n",
    "                if counter == 0:\n",
    "                    counter += 1\n",
    "                    pass\n",
    "                else:\n",
    "                    filename = \"train/\" + row[1]\n",
    "                    filenames.append(filename)\n",
    "                    label = row[2]\n",
    "                    labels.append(int(label))\n",
    "    \n",
    "    return filenames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_format(input_queue, possible_labels):\n",
    "    file_contents = tf.read_file(input_queue[0])\n",
    "\n",
    "    example = tf.image.decode_jpeg(file_contents, channels=3)\n",
    "    example = tf.random_crop(example, size=[500, 500, 3])\n",
    "    example = tf.image.random_flip_left_right(example)\n",
    "    example = tf.image.random_brightness(example, max_delta=0.25)\n",
    "    example = tf.image.random_contrast(example, lower=0.75, upper=1.25)\n",
    "    example = tf.cast(example, tf.float32)\n",
    "    \n",
    "    raw_label = input_queue[1]\n",
    "    label = tf.one_hot(tf.where(tf.equal(possible_labels, raw_label))[0],\n",
    "                       depth = possible_labels.shape[0], on_value = 1,\n",
    "                       off_value = 0)\n",
    "    label = label[0]\n",
    "    return example, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(file_list, unique_labels, batch_size = 3,\n",
    "                   num_epochs = None, evaluation = False):   \n",
    "    image_list, label_list = read_labeled_image_list(file_list)\n",
    "    \n",
    "    images = tf.convert_to_tensor(image_list, dtype=tf.string)\n",
    "    labels = tf.convert_to_tensor(label_list, dtype=tf.int32)\n",
    "    \n",
    "    input_queue = tf.train.slice_input_producer([images, labels],\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            shuffle=True)\n",
    "    \n",
    "    example, label = read_file_format(input_queue, unique_labels)\n",
    "\n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 10\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [example, label], batch_size=batch_size, capacity=capacity, \n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_with_weight_decay(name, shape, stddev, wd):\n",
    "\n",
    "    var = tf.Variable(name=name, initial_value=tf.truncated_normal(shape=shape,\n",
    "                          stddev=stddev, dtype=tf.float32), dtype=tf.float32)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "    tensor_name = x.name\n",
    "    tensor_name = tensor_name.replace(':', '_')\n",
    "    tensor_name = tensor_name.replace('(', '_')\n",
    "    tensor_name = tensor_name.replace(')', '_')\n",
    "    tensor_name = tensor_name.replace(' ', '_')\n",
    "\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(images):\n",
    "    \n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[5, 5, 3, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_with_weight_decay('biases',\n",
    "                                             shape=[64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=None)\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv1)\n",
    "        \n",
    "    \n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.dropout(pool1, keep_prob=0.5, name='norm1')\n",
    "    \n",
    "      # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = variable_with_weight_decay('weights',\n",
    "                                             shape=[5, 5, 64, 64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=0.0)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = variable_with_weight_decay('biases',\n",
    "                                             shape=[64],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=None)\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        _activation_summary(conv2)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.dropout(conv2, keep_prob=0.5, name='norm2')\n",
    "\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                           strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    \n",
    "     # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        # Move everything into depth so we can perform a single matrix multiply.\n",
    "        pool2_len = np.prod(pool2.get_shape().as_list()[1:])\n",
    "        reshape = tf.reshape(pool2, [batch_size, pool2_len])\n",
    "        weights = variable_with_weight_decay('weights', shape=[pool2_len, 384],\n",
    "                                              stddev=0.04, wd=0.004)\n",
    "        biases = variable_with_weight_decay('biases',\n",
    "                                             shape=[384],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=None)\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases,\n",
    "                            name=scope.name)\n",
    "        _activation_summary(local3)\n",
    "\n",
    "      # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                              stddev=0.04, wd=0.004)\n",
    "        biases = variable_with_weight_decay('biases',\n",
    "                                             shape=[192],\n",
    "                                             stddev=5e-2,\n",
    "                                             wd=None)\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases,\n",
    "                            name=scope.name)\n",
    "        _activation_summary(local4)\n",
    "\n",
    "      # linear layer(WX + b),\n",
    "      # We don't apply softmax here because\n",
    "      # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "        # and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = variable_with_weight_decay('weights', [192, len(unique_labels)],\n",
    "                                              stddev=1/192.0, wd=0.0)\n",
    "        biases = variable_with_weight_decay('biases',\n",
    "                                            shape=[len(unique_labels)],\n",
    "                                            stddev=5e-2,\n",
    "                                            wd=None)\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases,\n",
    "                                name=scope.name)\n",
    "        _activation_summary(softmax_linear)\n",
    "        \n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    # Calculate the average cross entropy loss across the batch.\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "      labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    \n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "        l_name = l.name.replace(\":\", \"_\")\n",
    "        tf.summary.scalar(l_name + '_raw_', l)\n",
    "        tf.summary.scalar(l_name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss, global_step):\n",
    " \n",
    "    # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = num_examples_per_train_epoch / batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * num_epochs_to_decay)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(\n",
    "        initial_learning_rate, global_step,\n",
    "        decay_steps, learning_rate_decay_factor, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.AdagradOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        moving_average_decay, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x, y_):\n",
    "    y_hat = make_prediction(x)\n",
    "    correct = tf.equal(tf.argmax(y_hat,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    tf.summary.scalar('validation_accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_labels = create_labeled_set()\n",
    "unique_labels = unique_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = 'Train_List.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size, valid_size = partition_data(file_list, percent_validation=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_partition = 'Train_Partition.csv'\n",
    "valid_partition = 'Train_Partition.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global num_examples_per_train_epoch\n",
    "num_examples_per_train_epoch = train_size\n",
    "\n",
    "global num_epochs\n",
    "num_epochs= 1\n",
    "\n",
    "global batch_size\n",
    "batch_size = 3\n",
    "\n",
    "global moving_average_decay\n",
    "moving_average_decay = 0.9999     # The decay to use for the moving average.\n",
    "\n",
    "global num_epochs_to_decay\n",
    "num_epochs_to_decay = 350.0    # Epochs after which learning rate decays.\n",
    "\n",
    "global learning_rate_decay_factor\n",
    "learning_rate_decay_factor = 0.1  # Learning rate decay factor.\n",
    "\n",
    "global initial_learning_rate\n",
    "initial_learning_rate = 0.00005       # Initial learning rate.\n",
    "\n",
    "global momentum\n",
    "momentum = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = 'TF_Logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    train_example_feed, train_labels_feed = input_pipeline(\n",
    "        train_partition, unique_labels=unique_labels,\n",
    "        batch_size=batch_size, num_epochs=num_epochs)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 500, 500, 3])\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 11])\n",
    "    \n",
    "    y_hat = make_prediction(x)\n",
    "    \n",
    "    loss = calculate_loss(y_hat, y_)\n",
    "    \n",
    "    train_op = train(loss, global_step=global_step)\n",
    "    step = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Start populating the filename queue.\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "        \n",
    "\n",
    "        while not coord.should_stop():\n",
    "            try:\n",
    "                start_time = time.time()                \n",
    "                \n",
    "                example_batch, label_batch = sess.run([train_example_feed,\n",
    "                                                       train_labels_feed])\n",
    "\n",
    "                result, summary =  sess.run([train_op, merged],\n",
    "                                            feed_dict={x: example_batch,\n",
    "                                                       y_: label_batch})\n",
    "                                \n",
    "                \n",
    "                \n",
    "                writer.add_summary(summary, global_step.eval())\n",
    "\n",
    "                step += 1\n",
    "                print(step)\n",
    "                duration = time.time() - start_time\n",
    "                print(duration)\n",
    "\n",
    "            except (tf.errors.OutOfRangeError,\n",
    "                    tf.errors.InvalidArgumentError) as e:\n",
    "                \n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "                \n",
    "                valid_example_feed, valid_labels_feed = input_pipeline(\n",
    "                    train_partition, unique_labels=unique_labels,\n",
    "                    batch_size=valid_size, num_epochs=1\n",
    "                )\n",
    "                \n",
    "                valid_examples, valid_labels = sess.run([valid_example_feed,\n",
    "                                                       valid_labels_feed])\n",
    "                \n",
    "                sess.run(evaluate_accuracy, feed_dict={x: valid_examples,\n",
    "                                                       y_: valid_labels})\n",
    "                summary = sess.run(merged,\n",
    "                                   feed_dict={\n",
    "                                       x: valid_examples, y_: valid_labels\n",
    "                                   })\n",
    "\n",
    "                \n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
